
@inproceedings{taghibakhshi_mg-gnn_2023,
	title = {{MG}-{GNN}: {Multigrid} {Graph} {Neural} {Networks} for {Learning} {Multilevel} {Domain} {Decomposition} {Methods}},
	copyright = {All rights reserved},
	shorttitle = {{MG}-{GNN}},
	url = {https://proceedings.mlr.press/v202/taghibakhshi23a.html},
	abstract = {Domain decomposition methods (DDMs) are popular solvers for discretized systems of partial differential equations (PDEs), with one-level and multilevel variants. These solvers rely on several algorithmic and mathematical parameters, prescribing overlap, subdomain boundary conditions, and other properties of the DDM. While some work has been done on optimizing these parameters, it has mostly focused on the one-level setting or special cases such as structured-grid discretizations with regular subdomain construction. In this paper, we propose multigrid graph neural networks (MG-GNN), a novel GNN architecture for learning optimized parameters in two-level DDMs. We train MG-GNN using a new unsupervised loss function, enabling effective training on small problems that yields robust performance on unstructured grids that are orders of magnitude larger than those in the training set. We show that MG-GNN outperforms popular hierarchical graph network architectures for this optimization and that our proposed loss function is critical to achieving this improved performance.},
	language = {en},
	urldate = {2024-04-01},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Taghibakhshi, Ali and Nytko, Nicolas and Zaman, Tareq Uz and Maclachlan, Scott and Olson, Luke and West, Matthew},
	month = jul,
	year = {2023},
	note = {ISSN: 2640-3498},
	pages = {33381--33395},
	file = {Full Text PDF:files/10/Taghibakhshi et al. - 2023 - MG-GNN Multigrid Graph Neural Networks for Learni.pdf:application/pdf},
}
